{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.layers as layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from os.path import abspath, join, pardir, dirname, lexists\n",
    "from src.data_cleaning import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "OOV_TOKEN = '<OOV>'\n",
    "TRUNCATING = 'pre'\n",
    "PADDING = 'pre'\n",
    "MAX_SENTENCE_LENGTH = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('input/train.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.question1 = train_data.question1.astype(str)\n",
    "train_data.question2 = train_data.question2.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlapping_ratio(row):\n",
    "    q1words = set()\n",
    "    q2words = set()\n",
    "    for word in row.question1.lower().split(' '):\n",
    "        q1words.add(word)\n",
    "    for word in row.question2.lower().split(' '):\n",
    "        q2words.add(word)\n",
    "    overlapping_words = q1words.intersection(q2words)\n",
    "    return round(len(overlapping_words) * 2/(len(q1words) + len(q2words)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = train_data.apply(get_overlapping_ratio, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.hist(overlaps[train_data.is_duplicate == 0], bins=40, label='different')\n",
    "plt.hist(overlaps[train_data.is_duplicate == 1], bins=40, alpha=0.7, label='duplicate')\n",
    "plt.legend()\n",
    "plt.xlabel('overlap ratio')\n",
    "plt.ylabel('number of instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = train_data.apply(lambda x: abs(len(x.question1) - len(x.question2))/max(len(x.question1),len(x.question2)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train = train_data.groupby('is_duplicate').apply(lambda x: x.sample(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "d1 = diffs[balanced_train.loc[0].index]\n",
    "d2 = diffs[balanced_train.loc[1].index]\n",
    "plt.hist(d1, bins=50, label='different')\n",
    "plt.hist(d2, bins=50, alpha=0.7, label='duplicate')\n",
    "plt.yscale('linear')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_count(sentence):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "q1_sentences = train_data['question1'].apply(sent_tokenize)\n",
    "q2_sentences = train_data['question2'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(q1_sentences), (q1_sentences.apply(len)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs[train_data.is_duplicate == 0].mean(), diffs[train_data.is_duplicate == 0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs[train_data.is_duplicate == 1].mean(),diffs[train_data.is_duplicate == 1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.append({'qid1': 303951, 'qid2': 174363, 'question1': 'How can I create an Android app?',\n",
    "                                'question2': 'How can I develop android app?', 'is_duplicate': 1}, ignore_index=True)\n",
    "train_data = train_data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    return ' '.join([word for word in sentence.split(' ') if word not in eng_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.question1 = train_data.question1.apply(lambda x: remove_stopwords(x))\n",
    "train_data.question2 = train_data.question2.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data[['question1', 'question2']],\n",
    "                                                      train_data['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training samples: {}, validation samples: {}\".format(X_train.shape, X_valid.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=OOV_TOKEN, num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(X_train.question1.append(X_train.question2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_seqs(x):\n",
    "    seqs = tokenizer.texts_to_sequences(x)\n",
    "    return pad_sequences(seqs, maxlen=MAX_SENTENCE_LENGTH, truncating=TRUNCATING, padding=PADDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1_sents = convert_to_seqs(X_train.question1.copy())\n",
    "train_q2_sents = convert_to_seqs(X_train.question2.copy())\n",
    "valid_q1_sents = convert_to_seqs(X_valid.question1.copy())\n",
    "valid_q2_sents = convert_to_seqs(X_valid.question2.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training input: {}, validation input: {}\".format(train_q1_sents.shape, valid_q1_sents.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not lexists(\"model/saved_model.pb\"):\n",
    "\n",
    "    q1_input = layers.Input((MAX_SENTENCE_LENGTH,), sparse=False)\n",
    "\n",
    "    q1_embedding = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=200, input_length=MAX_SENTENCE_LENGTH)(\n",
    "                                    q1_input)\n",
    "    q1_recurrent = layers.Bidirectional(layer=layers.GRU(128))(q1_embedding)\n",
    "\n",
    "    q1_dense = layers.Dense(64, activation='relu')(q1_recurrent)\n",
    "\n",
    "    output_layer = layers.Dense(1, activation='sigmoid')(q1_dense)\n",
    "\n",
    "    complete_model = tf.keras.Model(inputs=q1_input, outputs=[output_layer])\n",
    "    print(complete_model.summary())\n",
    "    complete_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    input_array = np.concatenate([train_q1_sents, train_q2_sents], axis=1)\n",
    "    print(input_array.shape)\n",
    "    complete_model.fit([train_q1_sents, train_q2_sents], y_train, batch_size=256, epochs=5,\n",
    "                       validation_data=([valid_q1_sents, valid_q2_sents], y_valid))\n",
    "\n",
    "    complete_model.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.question2 = test_data.question2.fillna('How I what can learn android app development?')\n",
    "test_data.question1 = test_data.question1.fillna('How app development?')\n",
    "print(test_data.loc[[1046690,1461432,379205,817520,943911,1270024], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape, test_data.columns)\n",
    "test_q1_sents = convert_to_seqs(test_data.question1)\n",
    "test_q2_sents = convert_to_seqs(test_data.question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max number of words in question 1 = 127\n",
    "\n",
    "max number of words in question 2 = 237\n",
    "\n",
    "We will pad sequences to normalize the length to 120, truncating longer ones from the beginning and padding zeroes to shorter ones on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(np.concatenate([test_q1_sents, test_q2_sents], axis=1))\n",
    "submission = pd.DataFrame({'is_duplicate': predictions[:, 0], 'test_id': test_data.test_id})\n",
    "submission.to_csv('bi_gru_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions = model.predict(np.concatenate([train_q1_sents[:100], train_q2_sents[:100]], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, a in enumerate(zip(training_predictions, y_train[:100])):\n",
    "    print(i, a[0][0], a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
